#Pine cone and rag and vector basics
in pinecone is a vector database and what is vector , vector is a numeric way of expressing data like how to express the data for the computer to understand and map it accordingly like for a car it gives random number of 40 and red car 40.1 and blue 40.2 so this is how it works, so pinecone is the vector databases of these numbers and how they work ,
%pip install -qU langchain-pinecone pinecone-notebooks
%pip install -qU langchain-pinecone langchain-google-genai

from google.colab import userdata
import os
from pinecone import Pinecone, ServerlessSpec

# Set your Pinecone API key here. Either paste it directly or retrieve it from userdata or a secret manager.
os.environ["PINECONE_API_KEY"] = userdata.get("PINECONE_API_KEY")
pinecone_api_key = os.environ.get("PINECONE_API_KEY")

# Initialize the Pinecone client
pc = Pinecone(api_key=pinecone_api_key)
import time

index_name = "my-rag-project"  # change if desired

pc.create_index(
  name=index_name,
  dimension=768,
  metric="cosine",
  spec=ServerlessSpec(cloud="aws", region="us-east-1"),
)

index = pc.Index(index_name)
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import os

os.environ["GOOGLE_API_KEY"] = userdata.get("GOOGLE_API_KEY")
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

vector = embeddings.embed_query("This is a test query tasmeer.")

vector[:5]

from langchain_pinecone import PineconeVectorStore

vector_store = PineconeVectorStore(index=index, embedding=embeddings)
from langchain_core.documents import Document

document_1 = Document(
    page_content="I had chocalate chip pancakes and scrambled eggs for breakfast this morning.",
    metadata={"source": "tweet"},
)
from uuid import uuid4

from langchain_core.documents import Document

document_1 = Document(
    page_content="I had chocalate chip pancakes and scrambled eggs for breakfast this morning.",
    metadata={"source": "tweet"},
)

document_2 = Document(
    page_content="The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.",
    metadata={"source": "news"},
)

document_3 = Document(
    page_content="Building an exciting new project with LangChain - come check it out!",
    metadata={"source": "tweet"},
)

document_4 = Document(
    page_content="Robbers broke into the city bank and stole $1 million in cash.",
    metadata={"source": "news"},
)

document_5 = Document(
    page_content="Wow! That was an amazing movie. I can't wait to see it again.",
    metadata={"source": "tweet"},
)

document_6 = Document(
    page_content="Is the new iPhone worth the price? Read this review to find out.",
    metadata={"source": "website"},
)

document_7 = Document(
    page_content="The top 10 soccer players in the world right now.",
    metadata={"source": "website"},
)

document_8 = Document(
    page_content="LangGraph is the best framework for building stateful, agentic applications!",
    metadata={"source": "tweet"},
)

document_9 = Document(
    page_content="The stock market is down 500 points today due to fears of a recession.",
    metadata={"source": "news"},
)

document_10 = Document(
    page_content="I have a bad feeling I am going to get deleted :(",
    metadata={"source": "tweet"},
)

documents = [
    document_1,
    document_2,
    document_3,
    document_4,
    document_5,
    document_6,
    document_7,
    document_8,
    document_9,
    document_10,
]

from uuid import uuid4

from langchain_core.documents import Document
uuids = [str(uuid4()) for _ in range(len(documents))]

vector_store.add_documents(documents=documents, ids=uuids)
results = vector_store.similarity_search(
    "LangChain provides abstractions to make working with LLMs easy",
)
for res in results:
    print(f"* {res.page_content} [{res.metadata}]")
results = vector_store.similarity_search_with_score(
    "is cristiano roanldo the goat",
)
for res, score in results:
    print(f"* [SIM={score:3f}] {res.page_content} [{res.metadata}]")
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    # other params...
)
def answer_to_user(query: str):
  vector_results = vector_store.similarity_search(query, k=2)
  final_answer= llm.invoke(f"Answer this user query: {query}, Here are some references to answer {vector_results}")
  # Removed the incorrect instantiation of ChatGoogleGenerativeAI
  # The following line was unnecessary and caused the error
  # ChatGoogleGenerativeAI(results,query)
  return final_answer
answer_to_user("Langhcain provides abstractions to make working with LLMS easy")
